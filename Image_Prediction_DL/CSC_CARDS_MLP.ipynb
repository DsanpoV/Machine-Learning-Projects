{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kty_DqjIfU-z"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.random.set_seed(2024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDEV9VV0fU-0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def rgb_to_gray(images):\n",
        "    images = images.astype(float)\n",
        "    # Grayscale conversion formula: Y = 0.299R + 0.587G + 0.114B\n",
        "    grayscale_images = np.dot(images, [0.299, 0.587, 0.114])\n",
        "    return grayscale_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eO5LM5CnfU-0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "def define_paths(dir):\n",
        "    filepaths = []\n",
        "    labels = []\n",
        "    folds = os.listdir(dir)\n",
        "    for fold in folds:\n",
        "        foldpath = os.path.join(dir, fold)\n",
        "        filelist = os.listdir(foldpath)\n",
        "        for file in filelist:\n",
        "            fpath = os.path.join(foldpath, file)\n",
        "            filepaths.append(fpath)\n",
        "            labels.append(fold)\n",
        "    return filepaths, labels\n",
        "\n",
        "def create_dataset(tr_dir, val_dir, ts_dir):\n",
        "    # train dataframe\n",
        "    files, labels = define_paths(tr_dir)\n",
        "    arrays = []\n",
        "    i=0\n",
        "    for file in files:\n",
        "        img = load_img(file, target_size=(100, 100))\n",
        "        img_array = img_to_array(img)\n",
        "        arrays.append(img_array)\n",
        "\n",
        "    arrays = np.array(arrays)\n",
        "    arrays = rgb_to_gray(arrays)\n",
        "    arrays = arrays.reshape((len(arrays), 10000)) / 255\n",
        "\n",
        "    encoder = LabelEncoder()\n",
        "    labels = np.array(encoder.fit_transform(labels)).reshape((len(labels),))\n",
        "\n",
        "    unique_values, counts = np.unique(labels, return_counts=True)\n",
        "\n",
        "    print(arrays.shape, labels.shape)\n",
        "\n",
        "    X_train = arrays\n",
        "    y_train = labels\n",
        "\n",
        "\n",
        "    # valid dataframe\n",
        "    files, labels = define_paths(val_dir)\n",
        "    arrays = []\n",
        "    for file in files:\n",
        "        img = load_img(file, target_size=(100, 100))\n",
        "        img_array = img_to_array(img)\n",
        "        arrays.append(img_array)\n",
        "\n",
        "    arrays = np.array(arrays)\n",
        "    arrays = rgb_to_gray(arrays)\n",
        "    arrays = arrays.reshape((len(arrays), 10000)) / 255\n",
        "\n",
        "    labels = np.array(encoder.fit_transform(labels)).reshape((len(labels),))\n",
        "\n",
        "    print(arrays.shape, labels.shape)\n",
        "\n",
        "    X_valid = arrays\n",
        "    y_valid = labels\n",
        "\n",
        "    # test dataframe\n",
        "    files, labels = define_paths(ts_dir)\n",
        "    arrays = []\n",
        "    for file in files:\n",
        "        img = load_img(file, target_size=(100, 100))\n",
        "        img_array = img_to_array(img)\n",
        "        arrays.append(img_array)\n",
        "\n",
        "    arrays = np.array(arrays)\n",
        "    arrays = rgb_to_gray(arrays)\n",
        "    arrays = arrays.reshape((len(arrays), 10000)) / 255\n",
        "\n",
        "    labels = np.array(encoder.fit_transform(labels)).reshape((len(labels),))\n",
        "\n",
        "    print(arrays.shape, labels.shape)\n",
        "\n",
        "    X_test = arrays\n",
        "    y_test = labels\n",
        "\n",
        "    return unique_values, counts, X_train, y_train, X_valid, y_valid, X_test, y_test, encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVTZUkbsfU-1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_images(X_train, y_train, encoder):\n",
        "    # Assuming the dataframe has 'image' column for images and 'label' column for labels\n",
        "    images = X_train\n",
        "    labels = encoder.inverse_transform(y_train)\n",
        "\n",
        "    classes = np.unique(labels)\n",
        "\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    rows = int(np.ceil(40 / 5))\n",
        "    for i in range(20):\n",
        "        plt.subplot(rows, 5, i + 1)\n",
        "        image = images[i+300]\n",
        "        plt.imshow(image.reshape(100,100,1))\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "dgq8tab9fU-2",
        "outputId": "fe99e3b7-a5d8-4044-fa0f-2cdee715aedc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/input/cards-image-datasetclassification/train'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a040972fcffb>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvalid_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/kaggle/input/cards-image-datasetclassification/valid'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/kaggle/input/cards-image-datasetclassification/test'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-654a8e7d7b86>\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(tr_dir, val_dir, ts_dir)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# train dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-654a8e7d7b86>\u001b[0m in \u001b[0;36mdefine_paths\u001b[0;34m(dir)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mfilepaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mfolds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mfoldpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/cards-image-datasetclassification/train'"
          ]
        }
      ],
      "source": [
        "train_dir = '/kaggle/input/cards-image-datasetclassification/train'\n",
        "valid_dir = '/kaggle/input/cards-image-datasetclassification/valid'\n",
        "test_dir = '/kaggle/input/cards-image-datasetclassification/test'\n",
        "classes, counts, X_train, y_train, X_valid, y_valid, X_test, y_test, encoder = create_dataset(train_dir, valid_dir, test_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKRFnJd1fU-2"
      },
      "outputs": [],
      "source": [
        "# Gráfico para visualizar a frequência de cada carta\n",
        "# Crie uma tabela de frequências\n",
        "print(\"Valor   Frequência\")\n",
        "print(\"------------------\")\n",
        "for value, count in zip(classes, counts):\n",
        "    print(f\"{value:5}   {count:10}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMaF1iwtfU-2"
      },
      "outputs": [],
      "source": [
        "# Escrever os valores da tabela anterior em linhas para que seja possível escrever corretamente no relatório (diretamente)\n",
        "\n",
        "all_labels = encoder.inverse_transform(classes)\n",
        "\n",
        "len_classes = len(classes)\n",
        "\n",
        "# Adicionar '&' entre os valores das contagens\n",
        "for i in range(1, len(all_labels) * 2 - 1, 2):\n",
        "    all_labels = np.insert(all_labels, i, '&')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKppR5jsfU-3"
      },
      "outputs": [],
      "source": [
        "print(all_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBI6V81ufU-3"
      },
      "outputs": [],
      "source": [
        "print(counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebSP3ypAfU-3"
      },
      "outputs": [],
      "source": [
        "show_images(X_train, y_train, encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6c7ezpV8fU-3"
      },
      "outputs": [],
      "source": [
        "# Dicionário de hiperparâmetros\n",
        "hiperparameters = {\n",
        "    'learning_rate': [0.001, 0.01],\n",
        "    'neurons': [128, 512],\n",
        "    'activation': ['relu', 'softmax'],\n",
        "    'epochs': [15, 30]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAcYKiGqfU-4"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "import time\n",
        "\n",
        "def build_and_train_model(learning_rate, neurons, activation, epochs):\n",
        "    model_MLP = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(neurons, activation=activation, input_shape=(10000,)),\n",
        "        tf.keras.layers.Dense(53, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model_MLP.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    inicio = time.time()\n",
        "    history_MLP = model_MLP.fit(X_train, y_train, epochs=epochs, validation_data = (X_valid, y_valid))\n",
        "    fim = time.time()\n",
        "    tempo_total = fim - inicio\n",
        "\n",
        "    test_loss, test_acc = model_MLP.evaluate(X_test, y_test)\n",
        "\n",
        "    return history_MLP, test_loss, test_acc, model_MLP, tempo_total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIMi1daKfU-4"
      },
      "outputs": [],
      "source": [
        "from itertools import product\n",
        "import math\n",
        "\n",
        "all_results = []\n",
        "best_model_MLP = [0,None,0,0]\n",
        "\n",
        "i = 1\n",
        "# Iterar sobre todas as combinações de hiperparâmetros\n",
        "for learning_rate, neurons, activation, epochs in product(*hiperparameters.values()):\n",
        "    print(f'*** Teste nº:{i} ***')\n",
        "    print(f\"Treinando modelo com learning_rate={learning_rate}, num_neurons={neurons}, funcao_de_ativacao={activation}, numero_de_epochs={epochs}\")\n",
        "    history_MLP, test_loss, test_acc, model_MLP, tempo_total = build_and_train_model(learning_rate, neurons, activation, epochs)\n",
        "    print(f'Accuracy no conjunto de teste: {test_acc}')\n",
        "    all_results.append([test_loss, test_acc, learning_rate, neurons, activation, epochs, tempo_total])\n",
        "\n",
        "    if best_model_MLP[3] < test_acc:\n",
        "        best_model_MLP = [i, history_MLP, test_loss, test_acc, model_MLP, learning_rate, neurons, activation, epochs, tempo_total]\n",
        "\n",
        "    i = i+1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDPNclSCfU-4"
      },
      "outputs": [],
      "source": [
        "best_model_MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PKOkjgGfU-4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "# Ensure that 'modelos_guardados' directory exists\n",
        "#os.makedirs('modelos_guardados', exist_ok=True)\n",
        "\n",
        "# Save the model in the specified directory\n",
        "model = best_model_MLP[4]\n",
        "model.save('best_model_CARDS_MLP.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azCsl-WWfU-4"
      },
      "outputs": [],
      "source": [
        "# Guardar todos os resultados num csv\n",
        "import csv\n",
        "import os\n",
        "\n",
        "csv_file_path = \"resultados_CARDS/tuning_results_CARDS_MLP.csv\"\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "os.makedirs(os.path.dirname(csv_file_path), exist_ok=True)\n",
        "\n",
        "fieldnames = [\"model_execution\", \"epochs\", \"learning_rate\", \"neurons\", \"activation_function\", \"test_loss\", \"accuracy\", \"tempo_execução\"]\n",
        "\n",
        "with open(csv_file_path, mode='w', newline='') as file:\n",
        "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "\n",
        "    model_execution = 1\n",
        "    for result in all_results:\n",
        "        [test_loss, test_acc, learning_rate, neurons, activation, epochs, tempo] = result\n",
        "        writer.writerow({\n",
        "            \"model_execution\": model_execution,\n",
        "            \"epochs\": epochs,\n",
        "            \"learning_rate\": learning_rate,\n",
        "            \"neurons\": neurons,\n",
        "            \"activation_function\": activation,\n",
        "            \"test_loss\": test_loss,\n",
        "            \"accuracy\": test_acc,\n",
        "            \"tempo_execução\": tempo\n",
        "        })\n",
        "        model_execution += 1\n",
        "\n",
        "print(\"CSV file created successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn1s4QPVfU-5"
      },
      "source": [
        "## Gráficos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YR_CQg92fU-5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_MLP = best_model_MLP[1]\n",
        "\n",
        "# Plotting accuracy\n",
        "plt.plot(history_MLP.history['accuracy'])\n",
        "plt.plot(history_MLP.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Treino', 'Validação'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Z16OHQdfU-5"
      },
      "outputs": [],
      "source": [
        "# Gráfico evolução Loss\n",
        "plt.plot(history_MLP.history['loss'])\n",
        "plt.plot(history_MLP.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Treino', 'Validação'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGRKTCdFfU-6"
      },
      "source": [
        "### Matriz de confusão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWEFo7EgfU-6"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "model_MLP = best_model_MLP[4]\n",
        "\n",
        "y_pred_fc = np.argmax(model_MLP.predict(X_test), axis=1)\n",
        "\n",
        "# confusion matrix\n",
        "cm_fc = confusion_matrix(y_true=y_test, y_pred=y_pred_fc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCVxs1XhfU-6"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Confusion Matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm_fc, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
        "plt.title('Matriz de Confusão')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}